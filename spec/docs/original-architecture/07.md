
## 7. Synchronization Workflow

With the architectural layers defined, we can now examine how the synchronization process operates. The synchronization process is governed by the **Sync Strategy** that the developer chooses.

1.  **Index Selection:** The application chooses which indices to sync based on its needs. For GroupedSync, this means subscribing to specific groups (e.g., "2024-08" for August shopping entries). For FullSync/OnDemandSync, this means syncing the entire FullIndex.
2.  **Index Synchronization:** The library fetches the selected index, reads its `idx:hasShard` list, and synchronizes the active shards.
3.  **App Notification (`onIndexUpdate`):** The library notifies the application with the list of headers from the synchronized index.
4.  **Sync Strategy Application:** Based on the configured strategy:
     - **FullSync:** Immediately fetch all resources listed in the index
     - **OnDemandSync:** Wait for explicit resource requests
5.  **On-Demand Fetch (`fetchFromRemote`):** When needed, the app calls `fetchFromRemote("https://alice.podprovider.org/data/shopping-entries/created/2024/08/weekly-shopping-001")`.
6.  **State-based Merge:** The library downloads the full RDF resource, consults the **Merge Contract**, performs property-by-property merging, and returns the merged object.
7.  **App Notification (`onUpdate`):** The library notifies the application with the complete, merged object for local storage.

### 7.1. Concrete Workflow Example

**Scenario:** OnDemandSync for recipe collection

```javascript
// 1. Index Selection: App requests recipe synchronization
await syncLibrary.syncDataType('schema:Recipe', { strategy: 'OnDemandSync' });

// 2. Index Synchronization: Library fetches recipe index and its shards
// Internal: GET https://alice.podprovider.org/indices/recipes/index-full-a1b2c3d4
// Internal: GET https://alice.podprovider.org/indices/recipes/index-full-a1b2c3d4/shard-mod-md5-2-0-v1_0_0
// Internal: GET https://alice.podprovider.org/indices/recipes/index-full-a1b2c3d4/shard-mod-md5-2-1-v1_0_0

// 3. App Notification: Library provides index headers for browsing
syncLibrary.onIndexUpdate((headers) => {
  console.log('Available recipes:', headers);
  // headers = [
  //   { iri: '.../tomato-basil-soup', name: 'Tomato Basil Soup', keywords: ['vegan', 'soup'] },
  //   { iri: '.../pasta-carbonara', name: 'Pasta Carbonara', keywords: ['pasta', 'italian'] }
  // ]
});

// 4. Sync Strategy Application: OnDemandSync waits for explicit requests

// 5. On-Demand Fetch: User clicks on recipe, app requests full data
const recipe = await syncLibrary.fetchFromRemote('https://alice.podprovider.org/data/recipes/tomato-basil-soup');

// 6. State-based Merge: Library downloads resource, applies CRDT merge rules
// Internal: GET https://alice.podprovider.org/data/recipes/tomato-basil-soup
// Internal: Consult merge contract at https://kkalass.github.io/meal-planning-app/crdt-mappings/recipe-v1
// Internal: Merge with local copy using LWW-Register and OR-Set algorithms

// 7. App Notification: Library provides merged recipe object
syncLibrary.onUpdate((mergedResource) => {
  console.log('Recipe ready for display:', mergedResource);
  // mergedResource = { name: 'Tomato Basil Soup', ingredients: [...], ... }
});
```

### 7.2. Management Phase Operations

Beyond regular data synchronization, the framework requires periodic **management operations** to maintain system health and clean up stale metadata. These operations are separate from normal sync workflows and run on a different schedule.

#### 7.2.1. Lazy Evaluation Principle

**Core Design Principle:** Management operations are performance-critical and must be implemented with high efficiency:

**Cheap Operations (Always Acceptable):**
- Cached index lookups (reading installation states from locally cached Framework Installation Index)
- Processing cached Framework Garbage Collection Index entries

**Critical Implementation Notes:** 
- **Caching Required:** "Cheap" operations rely on **local caching with ETag validation**. The framework maintains local copies of frequently-accessed indices (Installation Index, GC Index) and uses HTTP `If-None-Match` headers to get efficient `304 Not Modified` responses when data hasn't changed. Fresh HTTP requests for every management operation would violate the efficiency principle.
- **Frequency Management:** Management operations should **not run on every sync**. Recommended approach: run management operations only on the first sync of each day, or after extended periods of inactivity. This prevents unnecessary overhead during normal application usage.

**Lazy Operations (Only During Normal Access):**
- Reader list cleanup happens **only** when already syncing an index for other purposes
- Index deprecation happens **only** when accessing indices that are already being synchronized
- No dedicated scanning or fetching of resources solely for cleanup purposes

**Operations to Avoid:**
- Scanning Pod containers to discover resources for cleanup
- Fetching documents solely to check their cleanup eligibility  
- Any operations requiring O(total resources) or O(total indices) traversal
- Proactive "find and fix" patterns that traverse data structures

This principle ensures management operations remain efficient regardless of Pod data volume.

#### 7.2.2. Management Phase Scope and Frequency

**When Management Phase Runs:**
- **Scheduled**: Daily or weekly (configurable, default: daily)
- **No Coordination Required**: Each installation runs management operations independently without coordinating with other installations - CRDT merge semantics handle any concurrent operations safely

**Management Operations:**
1. **Installation Dormancy Detection**: Check installation activity and tombstone inactive ones (cheap: uses Framework Installation Index)
2. **Opportunistic Reader List Cleanup**: Remove tombstoned installations from `idx:readBy` lists during normal index sync operations (lazy: only for indices being synchronized)
3. **Opportunistic Index Deprecation**: Mark indices with no active readers as deprecated during normal access (lazy: only for indices being synchronized)
4. **Garbage Collection**: Process framework GC index for cleanup-ready resources (cheap: uses GC index, processes bounded batch sizes)

#### 7.2.3. Installation Index for Efficient Management

**Framework Installation Index:**
To avoid expensive Type Index container scanning, the framework maintains a dedicated installation index at `/indices/framework/installations-index-${hash}/index`.

**Index Properties:**
```turtle
<installations-index> a idx:FullIndex;
   sync:isGovernedBy mappings:index-v1;
   idx:indexesClass crdt:ClientInstallation;
   idx:indexedProperty [
     idx:trackedProperty crdt:lastActiveAt;        # For dormancy detection
     idx:readBy <installation-1>, <installation-2>
   ], [
     idx:trackedProperty crdt:maxInactivityPeriod; # For cleanup thresholds  
     idx:readBy <installation-1>, <installation-2>
   ];
   idx:shardingAlgorithm [
     a idx:ModuloHashSharding;
     idx:hashAlgorithm "md5";
     idx:numberOfShards 1
   ] .
```

**Benefits:**
- **Efficient batch validation**: Check all installation states in single index sync
- **No container scanning**: Avoid expensive Pod filesystem operations
- **Framework consistency**: Use same index patterns as user data

#### 7.2.4. Management Phase Algorithm

**Phase 1: Sync Installation Index (Cheap)**
1. Sync framework installation index (same as any other index)
2. Identify potentially dormant installations from `crdt:lastActiveAt` headers
3. Build priority list for validation (oldest first)

**Phase 2: Validate Dormant Installations (Cheap - Bounded by Installation Count)**
1. **Initial screening**: Use cached Installation Index to identify installations that appear dormant based on `crdt:lastActiveAt` headers
2. **Document validation**: For each potentially dormant installation:
   - GET installation document from Pod
   - Re-check dormancy using actual `crdt:lastActiveAt` vs `crdt:maxInactivityPeriod` from document
3. **Tombstone dormant installations**: If still dormant based on actual document data:
   - Apply document tombstone by adding `crdt:deletedAt` timestamp and performing universal emptying (remove all semantic content, keeping only framework metadata)
   - PUT updated installation document
4. **Automatic index registration**: Framework automatically registers tombstoned installation documents in Garbage Collection Index for eventual cleanup

**Phase 3: Framework Garbage Collection (Cheap - Uses GC Index)**
1. Process framework GC index for cleanup-ready documents
2. Remove documents beyond retention periods (bounded batch sizes)
3. Update GC index to reflect completed cleanups

**Phase 4: Opportunistic Cleanup (Lazy - Only During Normal Operations)**
1. **During subsequent normal sync operations**, not as part of management phase:
   - When syncing any index, opportunistically remove tombstoned installations from `idx:readBy` lists
   - When syncing any data document, opportunistically remove tombstoned installations from Hybrid Logical Clock entries
   - When syncing any index with empty reader lists, mark as deprecated
   - When syncing deprecated indices, apply tombstoning if appropriate

**Key Implementation Note:** Phase 4 operations are **not performed during management phase** - they happen lazily as part of normal data synchronization workflows.

#### 7.2.5. Coordination and Conflict Resolution

**Collaborative Execution**: Multiple installations may run management phases simultaneously. CRDT merge semantics ensure safe coordination:

- **Installation tombstoning**: OR-Set semantics on `crdt:deletedAt` allow multiple installations to safely mark dormant installations
- **Reader list updates**: OR-Set removal operations are commutative and convergent  
- **Index tombstoning**: OR-Set semantics on `crdt:deletedAt` ensure deterministic deletion state transitions

**Efficiency Optimization**: Management phase skips work already completed by other installations by checking index states before performing updates.

### 7.3. HTTP-Level Optimizations

**ETag-Based Conflict Detection:**

Solid Pods support standard HTTP ETags for optimistic concurrency control:
- **GET responses:** Include `ETag` header with resource version identifier
- **PUT requests:** Include `If-Match` header with stored ETag to detect concurrent modifications  
- **412 Precondition Failed:** Server rejects if ETag doesn't match current version

**CRDT Integration Strategy:**
Unlike traditional REST APIs that fail on conflicts, this framework leverages ETags as a performance optimization:
1. **Optimistic path:** Most PUTs succeed immediately when no concurrent modifications occurred
2. **Conflict resolution:** On 412 response, GET current state, perform CRDT merge with local changes, retry PUT
3. **Eventual consistency:** CRDT merge semantics ensure convergence regardless of update order

This approach provides both immediate conflict detection (via ETags) and robust conflict resolution (via CRDT merging), offering better performance than pure CRDT approaches while maintaining stronger consistency than traditional optimistic locking.
